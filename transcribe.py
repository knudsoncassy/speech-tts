import os import whisper from pathlib import Path DATA_DIR = Path(__file__).resolve().parent.parent / "data" WAV_DIR = DATA_DIR / "wavs" METADATA = DATA_DIR / "metadata.csv" # Load Whisper model directly on GPU with float16 print("[+] Loading Whisper large-v3 on CUDA...") model = whisper.load_model("large-v3", device="cuda", download_root=None) # uses your local cache if already downloaded def transcribe_all(): with open(METADATA, "w", encoding="utf-8") as f: for wav_file in sorted(WAV_DIR.glob("*.wav")): print(f"[+] Transcribing {wav_file.name}...") result = model.transcribe(str(wav_file), fp16=True) # GPU + FP16 text = result["text"].strip() f.write(f"{wav_file.stem}|{text}\n") print(f" â†’ {text}") if __name__ == "__main__": transcribe_all()
